{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание к уроку 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funct(loss,optimiser):\n",
    "    max_features = 20000\n",
    "\n",
    "    # обрезание текстов после данного количества слов (среди top max_features наиболее используемые слова)\n",
    "    maxlen = 80\n",
    "    batch_size = 50 # увеличьте значение для ускорения обучения\n",
    "\n",
    "    print('Загрузка данных...')\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "    print(len(x_train), 'тренировочные последовательности')\n",
    "    print(len(x_test), 'тестовые последовательности')\n",
    "\n",
    "    print('Pad последовательности (примеров в x единицу времени)')\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "    x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print('x_test shape:', x_test.shape)\n",
    "\n",
    "    print('Построение модели...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 128))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # стоит попробовать использовать другие оптимайзер и другие конфигурации оптимайзеров \n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optimiser,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print('Процесс обучения...')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=1, # увеличьте при необходимости\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test,\n",
    "                                batch_size=batch_size)\n",
    "    print('Результат при тестировании: ','loss ',loss,' optimiser:',optimiser , score)\n",
    "    print('Тестовая точность:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка данных...\n",
      "25000 тренировочные последовательности\n",
      "25000 тестовые последовательности\n",
      "Pad последовательности (примеров в x единицу времени)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Построение модели...\n",
      "Процесс обучения...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cats\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 84s 3ms/step - loss: 5.9605e-08 - accuracy: 0.4980 - val_loss: 5.9605e-08 - val_accuracy: 0.4953\n",
      "25000/25000 [==============================] - 16s 645us/step\n",
      "Результат при тестировании:  loss  <tensorflow.python.keras.losses.CategoricalCrossentropy object at 0x0000015AC6273388>  optimiser: adam 5.960465523457969e-08\n",
      "Тестовая точность: 0.495279997587204\n",
      "Загрузка данных...\n",
      "25000 тренировочные последовательности\n",
      "25000 тестовые последовательности\n",
      "Pad последовательности (примеров в x единицу времени)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Построение модели...\n",
      "Процесс обучения...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cats\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 82s 3ms/step - loss: 5.9605e-08 - accuracy: 0.5039 - val_loss: 5.9605e-08 - val_accuracy: 0.5068\n",
      "25000/25000 [==============================] - 16s 649us/step\n",
      "Результат при тестировании:  loss  <tensorflow.python.keras.losses.CategoricalCrossentropy object at 0x0000015AC6273388>  optimiser: sgd 5.960465523457969e-08\n",
      "Тестовая точность: 0.5067999958992004\n",
      "Загрузка данных...\n",
      "25000 тренировочные последовательности\n",
      "25000 тестовые последовательности\n",
      "Pad последовательности (примеров в x единицу времени)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Построение модели...\n",
      "Процесс обучения...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cats\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 84s 3ms/step - loss: 0.4588 - accuracy: 0.7838 - val_loss: 0.3774 - val_accuracy: 0.8341\n",
      "25000/25000 [==============================] - 16s 635us/step\n",
      "Результат при тестировании:  loss  binary_crossentropy  optimiser: adam 0.377449494689703\n",
      "Тестовая точность: 0.834119975566864\n",
      "Загрузка данных...\n",
      "25000 тренировочные последовательности\n",
      "25000 тестовые последовательности\n",
      "Pad последовательности (примеров в x единицу времени)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Построение модели...\n",
      "Процесс обучения...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cats\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 82s 3ms/step - loss: 0.6930 - accuracy: 0.5128 - val_loss: 0.6928 - val_accuracy: 0.5272\n",
      "25000/25000 [==============================] - 16s 623us/step\n",
      "Результат при тестировании:  loss  binary_crossentropy  optimiser: sgd 0.6927723662853241\n",
      "Тестовая точность: 0.5271999835968018\n"
     ]
    }
   ],
   "source": [
    "for loss in [tf.keras.losses.CategoricalCrossentropy(),'binary_crossentropy']:\n",
    "    for optimiser in ['adam','sgd',]:\n",
    "        funct(loss,optimiser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод:- Как и в прошлом дз  основное влияние оказывает количество эпох обучения чем больше тем лучше.\n",
    "- выбор оптимизатора пока adam показывает лучшие результаты, а также подбор loss функции лучший результат loss  binary_crossentropy  optimiser: adam \n",
    "- варьирование размером bath, также позваляет повысить точность или скорость обработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "193690/193690 [==============================] - 61s 315us/step - loss: 2.7810 - accuracy: 0.2560\n",
      "Epoch 2/25\n",
      "193690/193690 [==============================] - 60s 310us/step - loss: 2.3532 - accuracy: 0.3359\n",
      "Epoch 3/25\n",
      "193690/193690 [==============================] - 60s 312us/step - loss: 2.2027 - accuracy: 0.3713\n",
      "Epoch 4/25\n",
      "193690/193690 [==============================] - 62s 321us/step - loss: 2.1030 - accuracy: 0.3955\n",
      "Epoch 5/25\n",
      "193690/193690 [==============================] - 61s 315us/step - loss: 2.0306 - accuracy: 0.4146\n",
      "Epoch 6/25\n",
      "193690/193690 [==============================] - 61s 315us/step - loss: 1.9735 - accuracy: 0.4282\n",
      "Epoch 7/25\n",
      "193690/193690 [==============================] - 61s 316us/step - loss: 1.9289 - accuracy: 0.4391s - l - ETA: 2s - loss: 1.9289 - ac - ETA: 0s - loss: 1.9290 \n",
      "Epoch 8/25\n",
      "193690/193690 [==============================] - 61s 316us/step - loss: 1.8890 - accuracy: 0.4485\n",
      "Epoch 9/25\n",
      "193690/193690 [==============================] - 62s 320us/step - loss: 1.8560 - accuracy: 0.4566\n",
      "Epoch 10/25\n",
      "193690/193690 [==============================] - 62s 318us/step - loss: 1.8289 - accuracy: 0.4635\n",
      "Epoch 11/25\n",
      "193690/193690 [==============================] - 62s 318us/step - loss: 1.8054 - accuracy: 0.4709\n",
      "Epoch 12/25\n",
      "193690/193690 [==============================] - 61s 317us/step - loss: 1.7860 - accuracy: 0.4746\n",
      "Epoch 13/25\n",
      "193690/193690 [==============================] - 62s 318us/step - loss: 1.7667 - accuracy: 0.4795\n",
      "Epoch 14/25\n",
      "193690/193690 [==============================] - 61s 317us/step - loss: 1.7507 - accuracy: 0.4839\n",
      "Epoch 15/25\n",
      "193690/193690 [==============================] - 62s 323us/step - loss: 1.7346 - accuracy: 0.4882s - loss: 1.7344 - accuracy: 0.48 - ETA: 0s - loss: 1\n",
      "Epoch 16/25\n",
      "193690/193690 [==============================] - 63s 323us/step - loss: 1.7233 - accuracy: 0.4907\n",
      "Epoch 17/25\n",
      "193690/193690 [==============================] - 63s 325us/step - loss: 1.7097 - accuracy: 0.4937\n",
      "Epoch 18/25\n",
      "193690/193690 [==============================] - 64s 332us/step - loss: 1.6973 - accuracy: 0.4959\n",
      "Epoch 19/25\n",
      "193690/193690 [==============================] - 72s 374us/step - loss: 1.6859 - accuracy: 0.4986\n",
      "Epoch 20/25\n",
      "193690/193690 [==============================] - 71s 368us/step - loss: 1.6791 - accuracy: 0.5015\n",
      "Epoch 21/25\n",
      "193690/193690 [==============================] - 68s 349us/step - loss: 1.6694 - accuracy: 0.5042\n",
      "Epoch 22/25\n",
      "193690/193690 [==============================] - 63s 326us/step - loss: 1.6625 - accuracy: 0.5045\n",
      "Epoch 23/25\n",
      "193690/193690 [==============================] - 64s 329us/step - loss: 1.6522 - accuracy: 0.5079\n",
      "Epoch 24/25\n",
      "193690/193690 [==============================] - 63s 325us/step - loss: 1.6465 - accuracy: 0.5099\n",
      "Epoch 25/25\n",
      "193690/193690 [==============================] - 63s 324us/step - loss: 1.6385 - accuracy: 0.5112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x15ac662ff48>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Activation,Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# построчное чтение из примера с текстом \n",
    "with open(\"Alisyvstranechudes.txt\", 'rb') as _in:\n",
    "    lines = []\n",
    "    for line in _in:\n",
    "        line = line.strip().lower().decode(\"cp1251\", \"ignore\")\n",
    "     #  print(line)\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        lines.append(line)\n",
    "text = \" \".join(lines)\n",
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "\n",
    "\n",
    "# создание индекса символов и reverse mapping чтобы передвигаться между значениями numerical\n",
    "# ID and a specific character. The numerical ID will correspond to a column\n",
    "# ID и определенный символ. Numerical ID будет соответсвовать колонке\n",
    "# число при использовании one-hot кодировки для представление входов символов\n",
    "char2index = {c: i for i, c in enumerate(chars)}\n",
    "index2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# для удобства выберете фиксированную длину последовательность 10 символов \n",
    "SEQLEN, STEP = 10, 1\n",
    "input_chars, label_chars = [], []\n",
    "\n",
    "# конвертация data в серии разных SEQLEN-length субпоследовательностей\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i: i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])\n",
    "\n",
    "\n",
    "# Вычисление one-hot encoding входных последовательностей X и следующего символа (the label) y\n",
    "\n",
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# установка ряда метапамертров  для нейронной сети и процесса тренировки\n",
    "BATCH_SIZE, HIDDEN_SIZE = 64, 128\n",
    "NUM_ITERATIONS = 25 # 25 должно быть достаточно\n",
    "NUM_EPOCHS_PER_ITERATION = 25\n",
    "NUM_PREDS_PER_EPOCH = 100\n",
    "\n",
    "\n",
    "\n",
    "# Create a super simple recurrent neural network. There is one recurrent\n",
    "# layer that produces an embedding of size HIDDEN_SIZE from the one-hot\n",
    "# encoded input layer. This is followed by a Dense fully-connected layer\n",
    "# across the set of possible next characters, which is converted to a\n",
    "# probability score via a standard softmax activation with a multi-class\n",
    "# cross-entropy loss function linking the prediction to the one-hot\n",
    "# encoding character label.\n",
    "\n",
    "'''\n",
    "Создание очень простой рекуррентной нейронной сети. В ней будет один реккурентный закодированный входной слой. За ним последует полносвязный слой связанный с набором возможных следующих символов, которые конвертированы в вероятностные результаты через стандартную softmax активацию с multi-class cross-encoding loss функцию ссылающуются на предсказание one-hot encoding лейбл символа\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n",
    "        HIDDEN_SIZE,\n",
    "        return_sequences=False,\n",
    "        input_shape=(SEQLEN, nb_chars),\n",
    "        recurrent_dropout=0.2,\n",
    "        \n",
    "       # recurrent_initializer='glorot_uniform'\n",
    "    )\n",
    ")\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Итерация #: 0\n",
      "Epoch 1/1\n",
      "193690/193690 [==============================] - 60s 310us/step - loss: 1.6316 - accuracy: 0.5129\n",
      "Генерация из посева: а так креп\n",
      "а так крепко просто все подумала Алиса. – Вы все же подумала Алиса. – Вы все же подумала Алиса. – Вы все же по==================================================\n",
      "Итерация #: 1\n",
      "Epoch 1/1\n",
      "193690/193690 [==============================] - 60s 311us/step - loss: 1.6249 - accuracy: 0.5135\n",
      "Генерация из посева: – Рубите е\n",
      "– Рубите ей под немного стояла с ней подомно подумала Алиса. – Вы мне было не подумала Алиса. – Вы мне было не==================================================\n",
      "Итерация #: 2\n",
      "Epoch 1/1\n",
      "193690/193690 [==============================] - 61s 316us/step - loss: 1.6197 - accuracy: 0.5172s - loss: 1.6197 - \n",
      "Генерация из посева: е было, но\n",
      "е было, но не подумала Алиса. – Вы все равно не подумала Алиса. – Вы все равно не подумала Алиса. – Вы все рав==================================================\n",
      "Итерация #: 3\n",
      "Epoch 1/1\n",
      "193690/193690 [==============================] - 60s 312us/step - loss: 1.6148 - accuracy: 0.5168\n",
      "Генерация из посева:  планы. – \n",
      " планы. – Нет, но не подумала Алиса. – Вы все равно подумала Алиса. – Вы все равно подумала Алиса. – Вы все ра==================================================\n",
      "Итерация #: 4\n",
      "Epoch 1/1\n",
      "193690/193690 [==============================] - 61s 313us/step - loss: 1.6065 - accuracy: 0.5200s - loss: 1.6065 - accuracy: \n",
      "Генерация из посева: глись подо\n",
      "глись подобраться на столка и произнес на столки и произнес на столки и произнес на столки и произнес на столк\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NUM_ITERATIONS = 5 # 25 должно быть достаточно\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "NUM_PREDS_PER_EPOCH = 100\n",
    "# выполнение серий тренировочных и демонстрационных итераций \n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "\n",
    "    # для каждой итерации запуск передачи данных в модель \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Итерация #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "\n",
    "    # Select a random example input sequence.\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "\n",
    "    # для числа шагов предсказаний использование текущей тренируемой модели \n",
    "    # конструирование one-hot encoding для тестирования input и добавление предсказания.\n",
    "    print(\"Генерация из посева: %s\" % (test_chars))\n",
    "   # print(X_test[test_idx:test_idx+5])\n",
    "    print(test_chars, end=\"\")\n",
    "  \n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "\n",
    "        # здесь one-hot encoding.\n",
    "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for j, ch in enumerate(test_chars):\n",
    "            X_test[0, j, char2index[ch]] = 1\n",
    "\n",
    "        # осуществление предсказания с помощью текущей модели.\n",
    "        pred = model.predict(X_test, verbose=0)[0]\n",
    "       \n",
    "        y_pred = index2char[np.argmax(pred)]\n",
    "\n",
    "        # вывод предсказания добавленного к тестовому примеру \n",
    "        print(y_pred, end=\"\")\n",
    "\n",
    "        # инкрементация тестового примера содержащего предсказание\n",
    "        test_chars = test_chars[1:] + y_pred\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: взят текст Алиса в стране чудем на русском, подбор оптимизаторов лучший результат у adam loss categorical_crossentropy, применение dropout понизило точность. основной способ улучшения увеличение количества эпох. также как вариант отслеживание градиента потерь в модификации loss функции(в данной работе не применялось), проверено GDU, LSTM лучший результат LSTM в следствии большей последовательности.\n",
    "с генерированный текст\n",
    "а так креп\n",
    "а так крепко просто все подумала Алиса. – Вы все же подумала Алиса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[3.45638663]\n",
      "Pred:[0 0 0 0 0 0 0 1]\n",
      "True:[0 1 0 0 0 1 0 1]\n",
      "9 + 60 = 1\n",
      "------------\n",
      "Error:[3.63389116]\n",
      "Pred:[1 1 1 1 1 1 1 1]\n",
      "True:[0 0 1 1 1 1 1 1]\n",
      "28 + 35 = 255\n",
      "------------\n",
      "Error:[3.91366595]\n",
      "Pred:[0 1 0 0 1 0 0 0]\n",
      "True:[1 0 1 0 0 0 0 0]\n",
      "116 + 44 = 72\n",
      "------------\n",
      "Error:[3.72191702]\n",
      "Pred:[1 1 0 1 1 1 1 1]\n",
      "True:[0 1 0 0 1 1 0 1]\n",
      "4 + 73 = 223\n",
      "------------\n",
      "Error:[3.5852713]\n",
      "Pred:[0 0 0 0 1 0 0 0]\n",
      "True:[0 1 0 1 0 0 1 0]\n",
      "71 + 11 = 8\n",
      "------------\n",
      "Error:[2.53352328]\n",
      "Pred:[1 0 1 0 0 0 1 0]\n",
      "True:[1 1 0 0 0 0 1 0]\n",
      "81 + 113 = 162\n",
      "------------\n",
      "Error:[0.57691441]\n",
      "Pred:[0 1 0 1 0 0 0 1]\n",
      "True:[0 1 0 1 0 0 0 1]\n",
      "81 + 0 = 81\n",
      "------------\n",
      "Error:[1.42589952]\n",
      "Pred:[1 0 0 0 0 0 0 1]\n",
      "True:[1 0 0 0 0 0 0 1]\n",
      "4 + 125 = 129\n",
      "------------\n",
      "Error:[0.47477457]\n",
      "Pred:[0 0 1 1 1 0 0 0]\n",
      "True:[0 0 1 1 1 0 0 0]\n",
      "39 + 17 = 56\n",
      "------------\n",
      "Error:[0.21595037]\n",
      "Pred:[0 0 0 0 1 1 1 0]\n",
      "True:[0 0 0 0 1 1 1 0]\n",
      "11 + 3 = 14\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "import copy, numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# вычислим сигмоиду\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# конвертируем значение функции сигмоиды в ее производную. \n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "# генерация тренировочного датасета\n",
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([list(range(largest_number))],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "# входные переменные\n",
    "alpha = 0.1\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "# инициализация весов нейронной сети\n",
    "synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1\n",
    "synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1\n",
    "synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1\n",
    "\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)\n",
    " \n",
    "# тренировочная логика\n",
    "for j in range(10000):\n",
    "   \n",
    "    # генерация простой проблемы сложения (a + b = c)\n",
    "    a_int = np.random.randint(largest_number/2) # int version\n",
    "    a = int2binary[a_int] # бинарное кодирование\n",
    "\n",
    "    b_int = np.random.randint(largest_number/2) # int version\n",
    "    b = int2binary[b_int] # бинарное кодирование\n",
    "\n",
    "    # правильный ответ\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "    \n",
    "    # место где мы располагаем наши лучше результаты (бинарно закодированные)\n",
    "    d = np.zeros_like(c)\n",
    "\n",
    "    overallError = 0\n",
    "    if j%5:\n",
    "        layer_2_deltas = list()\n",
    "        layer_1_values = list()\n",
    "    layer_1_values.append(np.zeros(hidden_dim))\n",
    "    \n",
    "    # движение вдоль позиций бинарной кодировки\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        # генерация input и output\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # внутренний слой (input ~+ предыдущий внутренний)\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "\n",
    "        # output layer (новое бинарное представление)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # проверка упустили ли мы что-то и если да, то как много \n",
    "        layer_2_error = y - layer_2\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))\n",
    "        overallError += np.abs(layer_2_error[0])\n",
    "          # декодируем оценку чтобы мы могли ее вывести на экран\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "        \n",
    "        # сохраняем внутренний слой, чтобы мы могли его использовать в след. timestep\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "    \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "        prev_layer_1 = layer_1_values[-position-2]\n",
    "        \n",
    "        # величина ошибки в output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        # величина ошибки в hidden layer\n",
    "        \n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        # обновление всех весов и пробуем заново\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "    \n",
    "\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha    \n",
    "\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "    \n",
    "    # вывод на экран процесса обучения\n",
    "    if(j % 1000 == 0):\n",
    "        print(\"Error:\" + str(overallError))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Как вариант случайным образом фиксировать веса слоев в процессе обучения для обучения незадейсвонных слоев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
